{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model - Semiconductor Capacity Utilization\n",
    "**INDENG 142A Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages\n",
    "!pip3 install pandas numpy xgboost scikit-learn matplotlib requests -q --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for data handling, modeling, and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data from FRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch economic data from federal reserve (fred)\n",
    "# caputl = semiconductor capacity utilization (target variable)\n",
    "# ipg = semiconductor production index\n",
    "# daup = us auto production\n",
    "series = {'CAPUTLG3344S': 'CAPUTL', 'IPG3344S': 'IPG', 'DAUPSA': 'DAUP'}\n",
    "data = {}\n",
    "\n",
    "for sid, name in series.items():\n",
    "    url = f\"https://fred.stlouisfed.org/graph/fredgraph.csv?id={sid}&cosd=2000-01-01&coed={datetime.today().strftime('%Y-%m-%d')}\"\n",
    "    r = pd.read_csv(StringIO(requests.get(url).text), parse_dates=['observation_date'], index_col='observation_date')\n",
    "    data[name] = pd.to_numeric(r[sid].replace('.', np.nan), errors='coerce')\n",
    "\n",
    "df = pd.DataFrame(data).dropna()\n",
    "print(f\"Loaded {len(df)} observations\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lag features - use past values to predict future\n",
    "df['CAPUTL_lag1'] = df['CAPUTL'].shift(1)  # 1 month ago\n",
    "df['CAPUTL_lag3'] = df['CAPUTL'].shift(3)  # 3 months ago\n",
    "df['IPG_lag1'] = df['IPG'].shift(1)\n",
    "df['IPG_lag3'] = df['IPG'].shift(3)\n",
    "df['DAUP_lag3'] = df['DAUP'].shift(3)\n",
    "\n",
    "# create rolling averages - smooth out noise\n",
    "df['CAPUTL_ma3'] = df['CAPUTL'].rolling(3).mean()\n",
    "df['IPG_ma3'] = df['IPG'].rolling(3).mean()\n",
    "\n",
    "# add seasonality features\n",
    "df['Month'] = df.index.month\n",
    "df['Quarter'] = df.index.quarter\n",
    "\n",
    "# interaction term - captures relationship between production and auto demand\n",
    "df['IPG_x_DAUP'] = df['IPG'] * df['DAUP']\n",
    "\n",
    "# drop rows with missing values from lagging\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the 10 features we'll use for prediction\n",
    "features = ['CAPUTL_lag1', 'CAPUTL_lag3', 'IPG_lag1', 'IPG_lag3', 'DAUP_lag3',\n",
    "            'CAPUTL_ma3', 'IPG_ma3', 'Month', 'Quarter', 'IPG_x_DAUP']\n",
    "\n",
    "# split data: train on pre-2020, test on 2020 onwards\n",
    "# this tests how well model handles covid-era disruptions\n",
    "X_train = df.loc[:'2019', features]\n",
    "y_train = df.loc[:'2019', 'CAPUTL']\n",
    "X_test = df.loc['2020':, features]\n",
    "y_test = df.loc['2020':, 'CAPUTL']\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters to search over\n",
    "# learning_rate = how fast model learns (lower = more conservative)\n",
    "# n_estimators = number of trees in ensemble\n",
    "# max_depth = how deep each tree can grow\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5]\n",
    "}\n",
    "\n",
    "# use time series cross-validation to find best parameters\n",
    "# this respects temporal order of data (no future data leakage)\n",
    "grid = GridSearchCV(\n",
    "    xgb.XGBRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=TimeSeriesSplit(n_splits=3),\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test set\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "# calculate error metrics\n",
    "# mae = average absolute error in percentage points\n",
    "# rmse = penalizes large errors more heavily\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Test MAE:  {mae:.4f}\")\n",
    "print(f\"Test RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature importance from trained model\n",
    "# shows which features contribute most to predictions\n",
    "importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': grid.best_estimator_.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create side-by-side plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# left plot: actual vs predicted over time\n",
    "ax1.plot(y_test.index, y_test, label='Actual', linewidth=2)\n",
    "ax1.plot(y_test.index, y_pred, '--', label='Predicted', linewidth=2)\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Capacity Utilization (%)')\n",
    "ax1.legend()\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# right plot: feature importance ranking\n",
    "ax2.barh(importance['Feature'], importance['Importance'])\n",
    "ax2.set_xlabel('Importance')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('xgboost_results.png', dpi=150)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
